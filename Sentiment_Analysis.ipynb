{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30152,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport random\nimport os\n\nfrom pyspark.sql import SparkSession \nfrom pyspark.ml  import Pipeline     \nfrom pyspark.sql import SQLContext  \nfrom pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import QuantileDiscretizer\nfrom pyspark.sql import functions as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_COLUMNS = StructType([\n    StructField(\"target\", StringType(), True),\n    StructField(\"ids\", StringType(), True),\n    StructField(\"date\", StringType(), True),\n    StructField(\"flag\", StringType(), True),\n    StructField(\"user\", StringType(), True),\n    StructField(\"text\", StringType(), True)])\n\nDATASET_ENCODING = \"ISO-8859-1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.appName('Tweets Sentiment').getOrCreate()\ndf = spark.read.csv('../input/sentiment140',header = 'False',schema=DATASET_COLUMNS)\nspark.sparkContext.setLogLevel('ERROR')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()\ndf.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(sparkDF,col):\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'http\\S+', ''))\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, '@\\w+', ''))\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, '#', ''))\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, 'RT', ''))\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, ':', ''))\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, '[^A-Za-z0-9]+', ' '))\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, '\\-', ''))\n    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, '[ ]+', ' '))\n    sparkDF = sparkDF.withColumn(col, F.trim(sparkDF[col]))\n\n    return sparkDF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = preprocessing(df,'text')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.groupby(\"user\").count().show()\ndf.groupBy('user').count().sort('count',ascending=False).show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\nidf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\nlabel_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\npipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n\npipelineFit = pipeline.fit(train_set)\ntrain_df = pipelineFit.transform(train_set)\nval_df = pipelineFit.transform(val_set)\ntrain_df.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(maxIter=100)\nlrModel = lr.fit(train_df)\npredictionsLojistic = lrModel.transform(val_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictionsLojistic.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictionsLojistic.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictionsLojistic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = predictionsLojistic.filter(predictionsLojistic.label == predictionsLojistic.prediction).count() / float(val_set.count())\naccuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pipelineFit.transform(test_set)\ntestPredictionsLogistic = lrModel.transform(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictionsLogistic.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = testPredictionsLogistic.filter(testPredictionsLogistic.label == testPredictionsLogistic.prediction).count() / float(test_set.count())\ntest_roc_auc = evaluator.evaluate(testPredictionsLogistic)\nprint(\"Logistic HashingTF Test Accuracy Score: {0:.4f}\".format(test_accuracy))\nprint(\"Logistic HashingTF Test ROC-AUC: {0:.4f}\".format(test_roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier()\nrfModel = rf.fit(train_df)\npredictionsForest = rfModel.transform(val_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictionsForest.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictionsForest = rfModel.transform(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictionsForest.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = testPredictionsForest.filter(testPredictionsForest.label == testPredictionsForest.prediction).count() / float(test_set.count())\ntest_roc_auc = evaluator.evaluate(testPredictionsForest)\nprint(\"Random Forest HashingTF Test Accuracy Score: {0:.4f}\".format(test_accuracy))\nprint(\"Random Forest HashingTF Test ROC-AUC: {0:.4f}\".format(test_roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import CountVectorizer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\ncv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\nidf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\nlabel_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n\nlr = LogisticRegression(maxIter=100)\npipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n\npipelineFit = pipeline.fit(train_set)\npredictions2 = pipelineFit.transform(val_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions2.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = predictions2.filter(predictions2.label == predictions2.prediction).count() / float(val_set.count())\nroc_auc = evaluator.evaluate(predictions2)\nprint(\"Logistic CountVectorizer Accuracy Score: {0:.4f}\".format(accuracy))\nprint(\"Logistic CountVectorizer ROC-AUC: {0:.4f}\".format(roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictions = pipelineFit.transform(test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictions.show(25)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = testPredictions.filter(testPredictions.label == testPredictions.prediction).count() / float(test_set.count())\ntest_roc_auc = evaluator.evaluate(testPredictions)\nprint(\"Logistic CountVectorizer Test Accuracy Score: {0:.4f}\".format(test_accuracy))\nprint(\"Logistic CountVectorizer Test ROC-AUC: {0:.4f}\".format(test_roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier()\npipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, rf])\npipelineFitrf = pipeline.fit(train_set)\npredictions2rf = pipelineFit.transform(val_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions2rf.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = predictions2rf.filter(predictions2rf.label == predictions2rf.prediction).count() / float(val_set.count())\nroc_auc = evaluator.evaluate(predictions2rf)\nprint(\"Random Forest CountVectorizer Accuracy Score: {0:.4f}\".format(accuracy))\nprint(\"Random Forest CountVectorizer ROC-AUC: {0:.4f}\".format(roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictionsRF = pipelineFitrf.transform(test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testPredictionsRF.show(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = testPredictionsRF.filter(testPredictionsRF.label == testPredictionsRF.prediction).count() / float(test_set.count())\ntest_roc_auc = evaluator.evaluate(testPredictionsRF)\nprint(\"Random Forest CountVectorizer Test Accuracy Score: {0:.4f}\".format(test_accuracy))\nprint(\"Random Forest CountVectorizer Test ROC-AUC: {0:.4f}\".format(test_roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}